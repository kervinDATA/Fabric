{"cells":[{"cell_type":"markdown","source":["## DQ010 ðŸ”¶ GX Context Preparation\n","\n",">  **Note**: this tutorial is provided for educational purposes, for members of the [Fabric Dojo community](https://skool.com/fabricdojo/about). All content contained within is protected by Copyright Â© law. Do not copy or re-distribute. \n","\n","In this tutorial, we will configure our Great Expectations Context for our project! This is an essential first step in the process. \n","\n","In the notebook below, we will , we will go through the following steps: \n","1. Register the data source (this could be a flat-file, a Spark DF (can be used to test Lakehouse tables or Warehouse tables also), or a Semantic Model. \n","2. Define a list of GX Expectations for that dataset\n","3. Create a validation definition which ties together the dataset and the expectations. \n","\n","And we will go through this process for the three different datasets (or rather, the same dataset, but at different stages in the pipeline): \n","1. The incoming raw file \n","2. A Spark Dataframe (that we will validate after we clean/transform our data, before writing it to a Lakehouse table)\n","\n","At the end of the notebook, we perform a refactoring exercises to make it as quick and easy as possible to register new datasets into your GX Context. \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3febc9fd-4d63-4d43-a3d9-d0b32d5b5d47"},{"cell_type":"markdown","source":["#### Prerequisites\n","- Connect this notebook to your DQ009_DQS_DataStore you created in  DQ009. \n","- Connect this notebook to the Environment you created in DQ009. \n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5efe41d6-bfb0-43cd-a6b0-4b47052ec0be"},{"cell_type":"code","source":["# import GX \n","import great_expectations as gx\n","import great_expectations.expectations as gxe\n","\n","# intialize our contxt\n","context = gx.get_context(mode=\"file\", project_root_dir=\"/lakehouse/default/Files\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e4c02c42-1105-4d3f-a265-d9bb0e752073"},{"cell_type":"markdown","source":["## Data source 1: Incoming CSV File \n","#### Step 1.1: Connect to the data source\n","We start by configuring our Incoming File data source - this is of type \"pandas filesystem\". GX uses pandas to load and validate files. \n","\n","Notice how this time, we are only defining the Batch Definition - we are not initializing the Batch, like we did in the fundamentals exercises. This is because for the moment, we're not actually doing any validation, simply defining what we want to validate in the future. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c532ee2b-fa55-42b0-aac6-2ac6ef42b44b"},{"cell_type":"code","source":["base_directory = \"/lakehouse/default/Files/landing_zone\"\n","file_name = \"property-sales-messy\"\n","\n","batch_definition = (\n","    context.data_sources\n","        .add_pandas_filesystem(name=\"DQS\", base_directory=base_directory)\n","        .add_csv_asset(name=file_name)\n",").add_batch_definition_path(name=f\"{file_name}_batch_definition\", path=f\"{file_name}.csv\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"134a06c9-0aca-4a0e-a7d5-c411e1355ab7"},{"cell_type":"markdown","source":["#### Step 1.2: Configure a single expectation\n","For the moment, we will only declare a single Expectation of our dataset. I just want to setup the entire end-to-end Context. \n","\n","In the future (in DQ014), we will explore in-depth which Expectations make sense for each dataset - and we can update our Context at that later point. For now the focus is on setting up an 'MVP' system. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50f6f900-32c8-4118-ac14-3bb4bf9a97d1"},{"cell_type":"code","source":["# create a list, and define individual Expectations within that list\n","expectation = gxe.ExpectTableColumnsToMatchSet(column_set=[\"SaleID\", \"Address\", \"Type\", \"City\", \"SalePriceUSD\", \"Agent\", \"Transaction_TS\"])\n","\n","# create an Expectation suite\n","suite = gx.ExpectationSuite(name=\"sales_incoming_expectations\")\n","\n","# add the (empty) Expectation Suite to your context \n","suite = context.suites.add(suite)\n","\n","# add the expectation to the suite\n","suite.add_expectation(expectation)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a8e5796-3618-40e7-bf8b-4de4473a1fee"},{"cell_type":"markdown","source":["#### Step 1.3: Create the validation definition\n","\n","The Validation Definition essentially binds together the Batch Definition and the Expectation Suite. \n","\n","The Validation Definition is important, because it is what is used in the 'runtime' scenario. We will look at this further below, but first, let's just define our Validation Definition - don't worry this one's quite simple! "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3517fc3-4237-43ea-80fc-6beaa9e26afb"},{"cell_type":"code","source":["# create a Validation Definition\n","validation_definition = gx.ValidationDefinition(\n","    data=batch_definition, suite=suite, name=\"sales_incoming_file_validation\"\n",")\n","\n","# add the Validation Definition to our context \n","context.validation_definitions.add(validation_definition)\n","\n","# check that it has been successfully added: \n","context.validation_definitions.get(\"sales_incoming_file_validation\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8576d174-9254-428b-b249-3f46a2002481"},{"cell_type":"markdown","source":["## Data Source 2: Clean Spark DF \n","Next, let's focus on setting up a Spark DF data source in our Context. We go through exactly te same 3-step process, the only real difference is the data source type. \n","\n","#### Step 2.1: Connect to the data source"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae938e2c-894a-40eb-b8c7-fc6bcf64a198"},{"cell_type":"code","source":["lakehouse_name = \"DQS_DataStore\"\n","table_name = \"clean_property_sales\"\n","\n","batch_definition  = (\n","    context.data_sources\n","        .add_spark(name=lakehouse_name) # this is my data source name (the Lakehouse) \n","        .add_dataframe_asset(name=table_name) # this is my Lakehouse table name\n","    ).add_batch_definition_whole_dataframe(f\"{table_name}_batch_definition\") # defining a batch of the whole dataframe"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0ec992ee-13c7-42b0-96e4-0d9e44b9f888"},{"cell_type":"markdown","source":["#### Step 2.2: Configure a single expectation\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"359e1cbe-480d-4373-871e-e1219dc8e5aa"},{"cell_type":"code","source":["# create a list, and define individual Expectations within that list\n","expectation = gxe.ExpectColumnValuesToBeBetween(column=\"SalePriceUSD\", min_value=100000, max_value=30000000)\n","\n","# create an Expectation suite\n","suite = gx.ExpectationSuite(name=\"clean_property_sales_df_expectations\")\n","\n","# add the (empty) Expectation Suite to your context \n","suite = context.suites.add(suite)\n","\n","# add the expectation to the suite\n","suite.add_expectation(expectation)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2bcdbbd2-8ee1-4fde-8bb3-601dfdaeb5d6"},{"cell_type":"markdown","source":["#### Step 2.3: Create the validation definition"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"43b2b41f-68f2-4299-971d-c0ef00ed6377"},{"cell_type":"code","source":["# create a Validation Definition\n","validation_definition = gx.ValidationDefinition(\n","    data=batch_definition, suite=suite, name=\"clean_property_sales_df_validation\"\n",")\n","\n","# add the Validation Definition to our context \n","context.validation_definitions.add(validation_definition)\n","\n","# check that it has been successfully added: \n","context.validation_definitions.get(\"clean_property_sales_df_validation\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1fe6965-b8f5-4f56-9503-2699a575374b"},{"cell_type":"markdown","source":["## Data Source 3: Semantic Model\n","Finally, we declare our semantic model data source - for this, we use `add_pandas()`. Why pandas? We will be using Semantic Link to access the data inside our semantic models, and Semantic Link provides us with an object of type Fabric Dataframe - this is based on the Pandas Dataframe. Therefore we can pass in the Fabric Dataframe into GX as a Pandas data source. \n","\n","Don't worry we'll explain this process in more detail in DQ013, for now, we just need to register the data source. \n","\n","#### Step 3.1: Connect to datasource "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ccabb80-60be-4513-b49d-ee5b0647c3d2"},{"cell_type":"code","source":["semantic_model_name = \"PropertySales_Semantic_Model\"\n","table_name = \"semantic_model_fact_sales\"\n","\n","batch_definition  = (\n","    context.data_sources\n","        .add_pandas(name=semantic_model_name) # this is my data source name (the Lakehouse) \n","        .add_dataframe_asset(name=table_name) # this is my Lakehouse table name\n","    ).add_batch_definition_whole_dataframe(f\"{table_name}_batch_definition\") # defining a batch of the whole dataframe"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09324d67-c6de-4f05-95a9-a403916334f5"},{"cell_type":"markdown","source":["#### Step 3.2: Configure a single expectation\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2404878f-fa71-4d21-9f1a-df757be7866d"},{"cell_type":"code","source":["# create a list, and define individual Expectations within that list\n","expectation = gxe.ExpectColumnValuesToBeUnique(column=\"SaleID\")\n","\n","# create an Expectation suite\n","suite = gx.ExpectationSuite(name=\"property_sales_semantic_model_expectations\")\n","\n","# add the (empty) Expectation Suite to your context \n","suite = context.suites.add(suite)\n","\n","# add the expectation to the suite\n","suite.add_expectation(expectation)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3865dd00-38b4-4146-8b32-4fc2cb413f8d"},{"cell_type":"markdown","source":["#### Step 3.3: Create a validation definition"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b5e8c7b-d795-4559-a0dc-c204990c05a8"},{"cell_type":"code","source":["# create a Validation Definition\n","validation_definition = gx.ValidationDefinition(\n","    data=batch_definition, suite=suite, name=\"property_sales_semantic_model_validation\"\n",")\n","\n","# add the Validation Definition to our context \n","context.validation_definitions.add(validation_definition)\n","\n","# check that it has been successfully added: \n","context.validation_definitions.get(\"property_sales_semantic_model_validation\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9efd0dfb-d463-4dc6-bc48-7abb4abe15e5"},{"cell_type":"markdown","source":["## Tutorial Bonus (â¬›): Refactoring the codebase to make it easier/ quicker to register any dataset\n","You might have noticed that there is some repition in the code needed to run through the three steps for registering new datasets. \n","\n","Let's refactor the code to make it quick and easy to register new datasets (from any source): "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5009e7c4-6ec2-4eed-bd7c-87ec1bb96543"},{"cell_type":"code","source":["# function to register add any new data source to the context\n","# import GX \n","import great_expectations as gx\n","import great_expectations.expectations as gxe\n","\n","# intialize our contxt\n","context = gx.get_context(mode=\"file\", project_root_dir=\"/lakehouse/default/Files\")\n","\n","def create_batch_definition(type, params): \n","    \"\"\" func to register any datasource in a GX Context\n","    Inputs: \n","        type - string - one of \"csv\", \"spark_df\" or \"semantic_model\"\n","        params - object - {\"datasource_name\": \"\", \"dataasset_name\":\"\", \"base_directory\":\"\" }\n","    Output: GX Batch Definition\n","    \"\"\"\n","\n","    batch_definition = None\n","    if type == 'csv': \n","        batch_definition = (\n","        context.data_sources\n","            .add_pandas_filesystem(name=params['datasource_name'], base_directory=params['base_directory'])\n","            .add_csv_asset(name=params['dataasset_name'])\n","        ).add_batch_definition_path(name=f\"{params['dataasset_name']}_batch_definition\", path=f\"{params['dataasset_name']}.csv\")\n","    \n","    elif type == 'spark_df': \n","        batch_definition  = (\n","            context.data_sources\n","                .add_spark(name=params['datasource_name']) \n","                .add_dataframe_asset(name=params['dataasset_name'])\n","            ).add_batch_definition_whole_dataframe(f\"{params['dataasset_name']}_batch_definition\")\n","\n","    elif type == 'semantic_model': \n","        batch_definition  = (\n","            context.data_sources\n","                .add_pandas(name=params['datasource_name']) \n","                .add_dataframe_asset(name=params['dataasset_name']) \n","            ).add_batch_definition_whole_dataframe(f\"{params['dataasset_name']}_batch_definition\")\n","    \n","    return batch_definition\n","\n","def register_any_datasource(type=None, params = {}, expectations_list=[]) -> None: \n","\n","    \"\"\" This functions registers any datasource in your GX Context \n","    Inputs: \n","        type - string - one of \"csv\", \"spark_df\" or \"semantic_model\"\n","        params - object - {\"datasource_name\": \"\", \"dataasset_name\":\"\", \"base_directory\":\"\" }\n","        expectations_list - list - a list of GX Expectations for that dataset.  \n","    \"\"\"\n","\n","    batch_definition = create_batch_definition(type, params)\n","\n","    # create an Expectation suite\n","    suite = gx.ExpectationSuite(name=f\"{params['dataasset_name']}_expectations\")\n","    suite = context.suites.add(suite)\n","    [suite.add_expectation(expectation) for expectation in expectations_list]\n","\n","    # create a Validation Definition\n","    validation_definition = gx.ValidationDefinition(data=batch_definition, suite=suite, name=f\"{params['dataasset_name']}_validation\")\n","    context.validation_definitions.add(validation_definition)\n","    context.validation_definitions.get(f\"{params['dataasset_name']}_validation\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"894baf58-0d29-460f-9e7a-cfbd0a36a955"},{"cell_type":"markdown","source":["## Run your function\n","_Note: if you run this after all the previous cells, it won't work, because you've already registered this data source in the GX Context._ \n","\n","_What you can do if you want to test it is: run context.data_sources.delete(\"DQS\") to delete our first data source, and then run the code below to generate it again._"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"163dba62-3703-438d-8c38-38cb4552e4b5"},{"cell_type":"code","source":["# run this if you want to test the register_any_datasource() function \n","context.data_sources.delete(\"DQS\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1d18a575-56c8-4b7f-91da-10267120624e"},{"cell_type":"code","source":["csv_params = {\"datasource_name\": \"DQS\", \"dataasset_name\":\"property-sales-messy\", \"base_directory\":\"/lakehouse/default/Files/landing_zone\" }\n","\n","expectations_list = [gxe.ExpectTableColumnsToMatchSet(column_set=[\"SaleID\", \"Address\", \"Type\", \"City\", \"SalePriceUSD\", \"Agent\", \"Transaction_TS\"])]\n","\n","register_any_datasource(type='csv', params=csv_params, expectations_list=expectations_list)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc6f6411-8f96-4945-b77c-2753978c9535"},{"cell_type":"markdown","source":["## END "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"885ec689-c05b-4130-9d5f-4ca325baa5f8"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{"environmentId":"69e4db09-b5d7-4ae8-b36f-bb904d1b0e7e","workspaceId":"24abee62-a042-437f-8ad6-d8b5d8fe5d4b"}}},"nbformat":4,"nbformat_minor":5}