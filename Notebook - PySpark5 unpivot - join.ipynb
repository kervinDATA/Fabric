{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b8a821-013d-4631-a32e-982402f0e403",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# LH011 ðŸ”¶ PySpark Drills 5: Reshaping Data\n",
    "\n",
    ">  **Note**: this tutorial is provided for educational purposes, for members of the [Fabric Dojo community](https://skool.com). All content contained within is protected by Copyright Â© law. Do not copy or re-distribute. \n",
    "\n",
    "Welcome to the 5th of five drill-style tutorials. The goal of this mini-series is to expose you to a wide variety of commonly used PySpark functions. \n",
    "\n",
    "In this 5th tutorial in the mini-series, we will look at reshaping data: denormalizing, joining, merging, and more! \n",
    "\n",
    "You'll be given an empty code cell to write the code for each drill. Try to complete each drill without the use of the walkthrough video, but it's there if you need it! \n",
    "\n",
    "#### Prerequisites\n",
    "1. You should already have a Lakehouse in your Fabric Workspace (from the previous exercise) - LH007_PropertyLH \n",
    "2. Load this notebook into your Fabric Workspace. Connect this notebook to the LH007_PropertyLH Lakehouse. \n",
    "3. Download the LH011_Datasets.zip file from the Skool tutorial page, unzip the folder, and upload all three files into the Lakehouse Files area: \n",
    "- `property_sales_new_system.csv` \n",
    "- `city_details.csv`\n",
    "- `agent_details.csv` \n",
    "\n",
    "\n",
    "#### Data loading from CSV\n",
    "\n",
    "**Run the script below to load the `property_sales_new_system.csv` file from `Files/` into a Spark dataframe.** \n",
    "\n",
    "Review the new structure of the property sales dataset. Your client changed the property sales source system, so now you are receiving the data in a different format. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea17fc1-fbf4-4a85-a731-11453a9ea074",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"true\")\n",
    "        .option(\"inferSchema\", True)\n",
    "        .load(\"Files/property_sales_new_system.csv\")\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccd9c7-5509-496f-85fd-4f4fb0309837",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Drill 5.1: Unpivoting data\n",
    "So hopefully, after seeing the data, you recognised the need for a bit of unpivoting - or turning a Wide dataset into a narrower dataset. \n",
    "\n",
    "As we have seen in other modules already, having our SalePriceUSD spread across three columns is not ideal. We need to get our SalePriceUSD columns into one column, and for that we can use Spark's UNPIVOT functionality. \n",
    "\n",
    "- PySpark: [df.unpivot()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.unpivot.html#pyspark.sql.DataFrame.unpivot)\n",
    "- Spark SQL: [UNPIVOT()](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-unpivot.html)\n",
    "\n",
    "Note: when I was testing both of these in Fabric, only the Spark SQL method worked for me, so I recommend using the Spark SQL. Plus it gives a good opportunity to look at the hand-off between PySpark and Spark SQL again. \n",
    "\n",
    "Using Spark SQL [UNPIVOT()](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-unpivot.html), **unpivot the three columns House_SalePriceUSD, Apartment_SalePriceUSD, DetachedHouse_SalePriceUSD into a single column called SalePriceUSD. Assign the resultant dataframe to a variable called sales_unpivoted.** \n",
    "\n",
    "- _Note 1: all other columns should remain the same._ \n",
    "- _Note 2: you will have to create a temporary view from your original `df` to be able to access it using Spark SQL._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422b3f5-8342-4236-9b2a-363893f93ef6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create a temporary view from your df\n",
    "df.createOrReplaceTempView(\"new_property_sales\")\n",
    "\n",
    "# using Spark SQL UNPIVOT() , create a single SalePriceUSD column. Assign the output to sales_unpivoted\n",
    "sales_unpivoted = spark.sql(\n",
    "    '''\n",
    "    SELECT * FROM new_property_sales\n",
    "        UNPIVOT (\n",
    "            SalePriceUSD FOR Type IN (House_SalePriceUSD, Apartment_SalePriceUSD, DetachedHouse_SalePriceUSD)\n",
    "        )\n",
    "    '''\n",
    ")\n",
    "\n",
    "display(sales_unpivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a35215-1904-41c0-8c50-cf752b5b2af4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Drill 5.2: Joining with other datasets\n",
    "\n",
    "You have been tasked to prepare a dataset for the data science team. They are looking to predict the SalePriceUSD for a property, given all the other information that might be known about a property. \n",
    "\n",
    "They have asked you to look into the source system and extract and then merge any new, relevant information into the `sales_unpivoted` dataset that you think might help to predict SalePrice USD. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf57ae5-a8d8-4ef4-8e7e-5805f3aa4f3a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "cities_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"true\")\n",
    "        .option(\"inferSchema\", True)\n",
    "        .load(\"Files/city_details.csv\")\n",
    ")\n",
    "\n",
    "display(cities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e6e51-f3aa-4f59-b87d-b069af79e4fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load the agent details dataset into agents_df dataframe\n",
    "agents_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\",\"true\")\n",
    "        .option(\"inferSchema\", True)\n",
    "        .load(\"Files/agent_details.csv\")\n",
    ")\n",
    "\n",
    "# display the agents_df dataframe \n",
    "display(agents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d00ab0-4b92-4155-b1ae-90180288f6b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "full_sales_dataset = (\n",
    "    sales_unpivoted\n",
    "        .join(agents_df, sales_unpivoted.Agent == agents_df.AgentName, \"left\")\n",
    "        .join(cities_df, \"City\", \"left\")\n",
    ")\n",
    "\n",
    "\n",
    "display(full_sales_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e8d64-caeb-4c89-aa10-3a17cb7b56bf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Drill 5.3: Final dataset cleanup \n",
    "\n",
    "After showing the dataset to you data scientist, they have requested a few small changes to the structure. They have asked you to: \n",
    "- remove the AgentID & AgentName columns \n",
    "- add a prefix of 'City' to the column that came from the city_details dataset - to make it clear what this data represents. \n",
    "\n",
    "Using whichever method you prefer, **make the required changes to your dataframe, and finally load the final dataframe into a Lakehouse Table (called `sales_ml_prep_data`), ready for consumption by the data science team:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2be35-d01e-446f-8424-a5467b329e05",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# drop the AgentId and AgentName columns\n",
    "final_df = full_sales_dataset.drop('AgentID', 'AgentName')\n",
    "\n",
    "# declare the renaming dictionary\n",
    "renames = {'AvgSalePrice': 'CityAvgSalePrice', 'MedianIncome': 'CityMedianIncome', 'Population':'CityPopulation'}\n",
    "\n",
    "# rename the columns \n",
    "final_df = final_df.withColumnsRenamed(renames)          \n",
    "\n",
    "# write the final_df to the Lakehouse \n",
    "final_df.write.format('delta').mode('overwrite').saveAsTable('sales_ml_prep_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901e352-aced-4fa6-8c04-f2521fd888aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### END"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
